{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60056091",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-02T20:15:12.824422Z",
     "iopub.status.busy": "2025-02-02T20:15:12.824023Z",
     "iopub.status.idle": "2025-02-02T20:15:13.780259Z",
     "shell.execute_reply": "2025-02-02T20:15:13.779319Z"
    },
    "papermill": {
     "duration": 0.961774,
     "end_time": "2025-02-02T20:15:13.782291",
     "exception": false,
     "start_time": "2025-02-02T20:15:12.820517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4aa9b88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T20:15:13.787688Z",
     "iopub.status.busy": "2025-02-02T20:15:13.787165Z",
     "iopub.status.idle": "2025-02-02T20:15:15.486661Z",
     "shell.execute_reply": "2025-02-02T20:15:15.485613Z"
    },
    "papermill": {
     "duration": 1.704029,
     "end_time": "2025-02-02T20:15:15.488545",
     "exception": false,
     "start_time": "2025-02-02T20:15:13.784516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class PandasHandler:\n",
    "    def __init__(self, data):\n",
    "        self.df = pd.DataFrame(data)\n",
    "        self.cols_containing_NaN = self._get_columns_with_nan()\n",
    "    \n",
    "    def _get_columns_with_nan(self):\n",
    "        return self.df.columns[self.df.isna().any()].tolist()\n",
    "\n",
    "    def get_df(self, columns=''):\n",
    "        if not columns:\n",
    "            return self.df\n",
    "        else:\n",
    "            return self.df[columns]\n",
    "\n",
    "    def get_col_with_nan(self):\n",
    "        return self._get_columns_with_nan()\n",
    "\n",
    "    def get_numeric_cols(self):\n",
    "        return self.df.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "    def get_text_cols(self):\n",
    "        return self.df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    def get_categoric_cols(self):\n",
    "        return self.df.select_dtypes(include=['category']).columns.tolist()\n",
    "    \n",
    "    def display_col_with_nan(self):\n",
    "        nan_count = self.df.isnull().sum()\n",
    "        print('total NaN:' + str(nan_count.sum()))\n",
    "        print(f'Fields containing NaN:    {nan_count[nan_count>0].sort_values(ascending=False)}')\n",
    "        return self.cols_containing_NaN\n",
    "    \n",
    "    def drop_row_with_nan_val(self, threshold=1,axis=0,inplace=True):\n",
    "        self.df.dropna(threshold=threshold,axis=axis,inplace=inplace)\n",
    "        return\n",
    "\n",
    "    def drop_col(self,colnames):\n",
    "        self.df = self.df.drop(columns=colnames)\n",
    "        return\n",
    "\n",
    "    def drop_all_cols_with_nan(self, perc_treshold=50):\n",
    "        remove_col_with_nan(self._get_columns_with_nan())\n",
    "        return\n",
    "\n",
    "    # Add max scaled col --> -1 to 1\n",
    "    def add_max_scaled_col(self,colname, group_by = '', prefix='max_scaled_'):\n",
    "        new_name = prefix + colname\n",
    "        if len(group_by)==0:\n",
    "            self.df[new_name] = self.df[colname]  / self.df[colname].abs().max()\n",
    "        else:\n",
    "            self.df['max_abs_grouped_' + colname] = self.df.groupby(group_by)[colname].transform(lambda x: x.abs().max())\n",
    "            self.df[new_name] = self.df[colname]/self.df['max_abs_grouped_' + colname] \n",
    "        return\n",
    "\n",
    "    # Add max scaled col --> 0 to 1\n",
    "    def add_normalized_col(self,colname, prefix='normalized_'):\n",
    "        new_name = prefix + colname\n",
    "        self.df[new_name] = (self.df[colname] - self.df[colname].min()) / (self.df[colname].max() - self.df[colname].min())\n",
    "        return\n",
    "\n",
    "    # Add standardized col --> mean 0 standard_dev 1\n",
    "    def add_standard_col(self,colname, prefix='standard_'):\n",
    "        new_name = prefix + colname\n",
    "        self.df[new_name] = (self.df[colname] - self.df[colname].mean()) / self.df[colname].std()     \n",
    "        return\n",
    "\n",
    "    def repair_na_cols_with_val(self, colname, val):\n",
    "        self.df[colname] = self.df[colname].fillna(val)\n",
    "        return\n",
    "\n",
    "    def get_avg_col(self, colname, group_by=''):\n",
    "        if group_by:\n",
    "            return self.df.groupby(group_by)[colname].transform('mean')\n",
    "        else:\n",
    "            return self.df[colname].mean()\n",
    "\n",
    "    def get_max_col(self, colname, group_by=''):\n",
    "        if group_by:\n",
    "            return self.df.groupby(group_by)[colname].transform('max')\n",
    "        else:\n",
    "            return self.df[colname].max()\n",
    "\n",
    "    def get_min_col(self, colname, group_by=''):\n",
    "        if group_by:\n",
    "            return self.df.groupby(group_by)[colname].transform('min')\n",
    "        else:\n",
    "            return self.df[colname].min()\n",
    "\n",
    "    def plot_line(self, col='', index=''):\n",
    "        plt.figure(figsize=(16,6))\n",
    "        data = self.df\n",
    "        if index:\n",
    "            data = data.set_index(index)    \n",
    "            \n",
    "        if col:\n",
    "            sns.lineplot(data=data[col])        \n",
    "        else:\n",
    "            sns.lineplot(data=data.df)        \n",
    "            \n",
    "        return\n",
    "\n",
    "    def plot_distribution_line(self, col):\n",
    "        plt.figure(figsize=(16,6))\n",
    "        data = self.df[col].sort_values(by=col).reset_index()\n",
    "        sns.lineplot(data=data)\n",
    "        return\n",
    "   \n",
    "   \n",
    "    def plot_distribution(self, cols):\n",
    "        fig, axes = plt.subplots(nrows=int(np.ceil(len(cols)/3)),ncols=3,figsize=(15,5))\n",
    "        axes = axes.flatten()\n",
    "        for i, column in enumerate(cols):\n",
    "            sns.histplot(self.df[column], kde=True, ax=axes[i])\n",
    "            axes[i].set_title(f'distrib {column}')\n",
    "            axes[i].set_xlabel(column)\n",
    "            axes[i].set_ylabel('Freq')\n",
    "\n",
    "        for j in range(i + 1, len(axes)):\n",
    "            axes[j].axis('off')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return\n",
    "\n",
    "def load_csv(file_path, parse_dates=True):\n",
    "    df = pd.read_csv(file_path,  parse_dates=parse_dates)\n",
    "    handler = PandasHandler(df)\n",
    "    return handler\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.01127,
   "end_time": "2025-02-02T20:15:16.211852",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-02T20:15:10.200582",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
