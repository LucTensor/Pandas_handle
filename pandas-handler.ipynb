{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "044c1c51",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-07T15:08:19.640754Z",
     "iopub.status.busy": "2025-02-07T15:08:19.640441Z",
     "iopub.status.idle": "2025-02-07T15:08:20.494218Z",
     "shell.execute_reply": "2025-02-07T15:08:20.493294Z"
    },
    "papermill": {
     "duration": 0.859634,
     "end_time": "2025-02-07T15:08:20.496136",
     "exception": false,
     "start_time": "2025-02-07T15:08:19.636502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b16f5e3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T15:08:20.503854Z",
     "iopub.status.busy": "2025-02-07T15:08:20.503395Z",
     "iopub.status.idle": "2025-02-07T15:08:23.033005Z",
     "shell.execute_reply": "2025-02-07T15:08:23.032215Z"
    },
    "papermill": {
     "duration": 2.535463,
     "end_time": "2025-02-07T15:08:23.034814",
     "exception": false,
     "start_time": "2025-02-07T15:08:20.499351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class PandasHandler:\n",
    "    \"\"\"\n",
    "    Handler for basic operations on dataframe pandas\n",
    "    visualization, nan inspection and repair\n",
    "    LOADING OF Dataframe\n",
    "    load_csv(file_path, parse_dates=True):\n",
    "    def get_df(self, columns=''):\n",
    "    def get_train_test_df(self, target='', features='', split_percentage=0.2):\n",
    "    def split_df(self, test_size=0.2, target_col):\n",
    "    \n",
    "    # Investigation\n",
    "    def get_col_with_nan(self):\n",
    "    def display_col_with_nan(self):\n",
    "    def count_categories(self):\n",
    "    def get_numeric_cols(self):\n",
    "    def get_text_cols(self):    \n",
    "    def get_categoric_cols(self):\n",
    "    def get_percStdDev_data(self, colname, limit_percStdDev=1):\n",
    "    def get_outliers(self, col):\n",
    "    def get_iqr_outliers(self, col):\n",
    "    def get_sigma_outliers(self, col):\n",
    "    def get_density_outliers(self, selected_columns=None, epsilon=1.5, min_samples=5, metric='euclidean'):\n",
    "    def get_df_variety(self, X=None, only_categorical=False):\n",
    "\n",
    "    # Repair\n",
    "    def repair_nan(self, colname=None, mode='value', value=0):\n",
    "    def drop_row_with_nan_val(self, threshold=1,axis=0,inplace=True):\n",
    "    def drop_col(self,colnames):\n",
    "    def categories_threshold(self, threshold=5, under_threshold=True):\n",
    "    def factorize_categories(self, columns=None):\n",
    "    def remove_outliers(self, col, method='sigma', epsilon=1.5, min_samples=5, metric='euclidean'):\n",
    "    def reduce_cat(self, col, values):\n",
    "\n",
    "    # Add utility cols\n",
    "    def standard_scaling(self, col1, col2):\n",
    "    def normalize_scaling(self, col1, col2):\n",
    "    def add_max_scaled_col(self,colname, group_by = '', prefix='max_scaled_'):\n",
    "    def add_normalized_col(self,colname, prefix='normalized_'):\n",
    "    def add_standard_col(self,colname, prefix='standard_'):\n",
    "    def mean(self, colname, group_by='', addCol=False):\n",
    "    def max(self, colname, group_by='', addCol=False):\n",
    "    def min(self, colname, group_by='', addCol=False):\n",
    "    def label_encode(self,columns=None):\n",
    "    def onehot_encode(self, columns=None):\n",
    "\n",
    "    # Visualization\n",
    "    def plot_line(self, y='', x=''):\n",
    "    def plot_distribution_line(self, col):\n",
    "    def distribution(self, cols, num_subplots_perrow=3):\n",
    "    def scatter(self, colx, coly, num_subplots_perrow=3, limit_percStdDev=1):\n",
    "    def correlations(self, annotation=True, limit=0):\n",
    "    def correlations(self, df=None, annotation=True, limit=0):\n",
    "    def boxplot_outliers(self, col):\n",
    "    def pairplot_relations(self):\n",
    "    def features_importance(self, X=None, y=None, n_features_to_vis=25):\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, data=None):\n",
    "        self.df = None\n",
    "        if data is not None:\n",
    "            self.df = pd.DataFrame(data)\n",
    "            self.cols_containing_NaN = self._get_columns_with_nan()\n",
    "    \n",
    "        self.features = None\n",
    "        self.target = None\n",
    "        self.X = self.df\n",
    "        self.y = None\n",
    "    \n",
    "    def _get_columns_with_nan(self):\n",
    "        return self.df.columns[self.df.isna().any()].tolist()\n",
    "\n",
    "    def get_X(self):\n",
    "        return self.X\n",
    "        \n",
    "    def get_y(self):\n",
    "        return self.y\n",
    "        \n",
    "    def set_features(self, features):\n",
    "        self.features = features\n",
    "        return\n",
    "        \n",
    "    def set_target(self, target):\n",
    "        self.target = target\n",
    "        self.features.pop(self.features.index(target))\n",
    "        self.X = self.df\n",
    "        self.y = self.X.pop(target)\n",
    "        return\n",
    "\n",
    "    def get_df_variety(self, X=None, only_categorical=False):\n",
    "        \"\"\"Gets the count of group by field of every field of the ds\"\"\"\n",
    "        if X is None:\n",
    "            X = self.df\n",
    "        \n",
    "        row_count = X.shape[0]\n",
    "        print(f'Total rows: {row_count}')\n",
    "        \n",
    "        for col in X.columns:\n",
    "            if X[col].dtype == 'object' or X[col].dtype == 'category':\n",
    "                counts = X.groupby(col)[col].count()\n",
    "                percentages = (counts / counts.sum()) * 100\n",
    "                \n",
    "                counts = counts.apply(np.floor).astype(int)\n",
    "                percentages = percentages.apply(np.floor).astype(int)\n",
    "    \n",
    "                print(f'\\nColonna: {col}')\n",
    "                print(f'{\"Nome Campo\":<15} {\"Conteggi\":<10} {\"Percentuale\":<12}')\n",
    "                for name, count, percent in zip(counts.index, counts, percentages):\n",
    "                    print(f'{name:<15} {count:<10} {percent:<12}')        \n",
    "            else:\n",
    "                print(f'{\"Nome Campo\":<15} {\"Conteggi\":<10} {\"Percentuale\":<12}')\n",
    "                counts = X[col].notnull().count()\n",
    "                percent = counts/row_count\n",
    "                print(f'{col:<15} {counts:<10} {percent:<12}')        \n",
    "                \n",
    "                    \n",
    "        return\n",
    "\n",
    "    def reduce_cat(self, X=None,  col=None, values=None):\n",
    "        \"\"\"Assigns Other if the value in col is not in values\"\"\"\n",
    "        if X is None:\n",
    "            X = self.df\n",
    "        other_val = 'Other'\n",
    "        if col is not None:\n",
    "            X[col] = X[col].apply(lambda x: x if x in values else other_val)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def print_numbercols_notzero(self, X=None):\n",
    "        \"\"\"Prints all numeric cols with their count>0\"\"\"        \n",
    "\n",
    "        if X is None:\n",
    "            X = self.df\n",
    "        cols = X.select_dtypes(include=['number'])\n",
    "\n",
    "        for col in cols:\n",
    "            print(f'Counts for {col}--> {X[X[col]>0][col].count()}')\n",
    "\n",
    "\n",
    "    \n",
    "    def isolate_features(self, X=None, features=None, target=None):\n",
    "        \"\"\"Isolates the features of a dataframe\"\"\"        \n",
    "\n",
    "        if X is None:\n",
    "            X = self.df\n",
    "        \n",
    "        if features is None:\n",
    "            features = X.columns.tolist()  # Converti in lista se None\n",
    "        \n",
    "        if isinstance(features, str):\n",
    "            features = [features]  # Trasforma in lista se è una stringa\n",
    "        \n",
    "        # Controllo se tutte le caratteristiche specificate esistono in X\n",
    "        missing_features = [feature for feature in features if feature not in X.columns]\n",
    "        if missing_features:\n",
    "            raise ValueError(f\"Le seguenti colonne non esistono nel DataFrame: {missing_features}\")\n",
    "        \n",
    "        if target is not None:\n",
    "            self.set_target(target)\n",
    "            # Estrai il target e rimuovilo da X\n",
    "            if target in X.columns:\n",
    "                self.y = X.pop(target)\n",
    "            else:\n",
    "                raise ValueError(f\"La colonna target '{target}' non esiste nel DataFrame.\")\n",
    "        \n",
    "        self.set_features(features)\n",
    "        \n",
    "        # Isola solo le colonne specificate e modifica direttamente X\n",
    "        for column in X.columns:\n",
    "            if column not in features:\n",
    "                del X[column]  # Rimuovi le colonne non specificate\n",
    "        \n",
    "        self.X = X  # Aggiorna self.X con il DataFrame modificato\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_df(self, columns=''):\n",
    "        \"\"\"Gets the dataframe\"\"\"\n",
    "        if not columns:\n",
    "            return self.df\n",
    "        else:\n",
    "            return self.df[columns]\n",
    "\n",
    "    def split_df(self , target_col, test_size=0.2):\n",
    "        \"\"\"Splits the df in X and y for test and train\"\"\"\n",
    "        X = self.df.copy()\n",
    "        y = X.pop(target_col)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size, random_state=42)        \n",
    "        return X_train, y_train, X_test, y_test\n",
    "    \n",
    "\n",
    "    def get_train_test_df(self, target=None, features=None, split_percentage=0.2):\n",
    "        \"\"\"Gets the pairs X, y for training and test\"\"\"\n",
    "        if target is not None and target not in self.df.columns:\n",
    "            raise ValueError(f\"Target '{target}' not found in DataFrame.\")\n",
    "        \n",
    "        train_df, test_df = train_test_split(\n",
    "            self.df, \n",
    "            test_size=split_percentage, \n",
    "            random_state=42, \n",
    "        )        \n",
    "        \n",
    "        if target is not None:\n",
    "            y_train = train_df.pop(target)\n",
    "            y_test = test_df.pop(target)\n",
    "        else:\n",
    "            y_train = []  # o pd.Series(dtype='float')\n",
    "            y_test = []   # o pd.Series(dtype='float')\n",
    "\n",
    "        if features is None:\n",
    "            features = self.df.columns.drop(target) if target else self.df.columns\n",
    "            \n",
    "        X_train = train_df[features]\n",
    "        X_test = test_df[features]\n",
    "\n",
    "        return X_train, y_train, X_test, y_test\n",
    "        \n",
    "    \n",
    "    def get_col_with_nan(self, X=None):\n",
    "        \"\"\"Gets all column names with nan and lists them with counting\"\"\"\n",
    "        if X is None:\n",
    "            X = self.get_df()\n",
    "            print(self._get_columns_with_nan())\n",
    "            \n",
    "        nan_columns = {}\n",
    "        total = X.shape[0]\n",
    "        \n",
    "        for column in X.columns:\n",
    "            nan_count = X[column].isnull().sum()\n",
    "            if nan_count > 0:\n",
    "                percentage = '{:.2f}'.format(nan_count*100 / total)\n",
    "                nan_columns[column] = f'{nan_count} count, {percentage}% of total'\n",
    "        \n",
    "        return nan_columns\n",
    "        \n",
    "    def repair_nan(self, X=None, colname=None, mode='value', value=0):\n",
    "        \"\"\"\n",
    "        Repair df where nan is found in columns \n",
    "        MODES: value, delrow, mostcommon, mean, min, max, random_number\n",
    "        \"\"\"\n",
    "        if X is None:\n",
    "            X = self.get_df()\n",
    "        \n",
    "        if colname is None:\n",
    "            colname = X.columns.tolist()\n",
    "\n",
    "        if isinstance(colname, str):\n",
    "            colname = [colname]\n",
    "\n",
    "        for col in colname:\n",
    "            if col not in X.columns:\n",
    "                print(f\"{col} not found DataFrame.\")\n",
    "                continue\n",
    "\n",
    "            if mode == 'value':\n",
    "                X[col].fillna(value, inplace=True)\n",
    "            elif mode == 'delrow':\n",
    "                X.dropna(subset=[col], inplace=True)\n",
    "            elif mode == 'mostcommon':\n",
    "                most_common = X[col].mode()[0] \n",
    "                X[col].fillna(most_common, inplace=True)\n",
    "            elif mode == 'mean':\n",
    "                mean_value = X[col].mean()\n",
    "                X[col].fillna(mean_value, inplace=True)\n",
    "            elif mode == 'min':\n",
    "                min_value = X[col].min()\n",
    "                X[col].fillna(min_value, inplace=True)\n",
    "            elif mode == 'max':\n",
    "                max_value = X[col].max()\n",
    "                X[col].fillna(max_value, inplace=True)\n",
    "            elif mode == 'random_number':\n",
    "                mean_value = X[col].mean()\n",
    "                std_dev = X[col].std()  \n",
    "                def is_nan(x):\n",
    "                    return pd.isna(x)\n",
    "                \n",
    "                def random_error(x):\n",
    "                    if is_nan(x):\n",
    "                        return random.gauss(mean_value, 2 * std_dev) \n",
    "                    return x\n",
    "            \n",
    "                X[col] = X[col].apply(random_error)            \n",
    "            else:\n",
    "                print(f'{mode} not found')\n",
    "\n",
    "        return\n",
    "\n",
    "    def get_numeric_cols(self):\n",
    "        \"\"\"Gets all numeric column names in a list\"\"\"\n",
    "        return self.df.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "    def get_text_cols(self):\n",
    "        \"\"\"Gets all text column names in a list\"\"\"\n",
    "        return self.df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    def get_categoric_cols(self):\n",
    "        \"\"\"Gets all categoric column names in a list\"\"\"\n",
    "        return self.df.select_dtypes(include=['category']).columns.tolist()\n",
    "    \n",
    "    def display_col_with_nan(self):\n",
    "        \"\"\"Displays total of nan counts, the columns containing nan\"\"\"\n",
    "        nan_count = self.df.isnull().sum()\n",
    "        print('total NaN:' + str(nan_count.sum()))\n",
    "        print(f'Fields containing NaN:    {nan_count[nan_count>0].sort_values(ascending=False)}')\n",
    "        return self.cols_containing_NaN\n",
    "    \n",
    "    def drop_row_with_nan_val(self, threshold=1,axis=0,inplace=True):\n",
    "        \"\"\"Drops rows having a nan count over the cols over a certain threshold. Ex:threshold=1\n",
    "        mantains only rows containing 1 nan\"\"\"\n",
    "        self.df.dropna(threshold=threshold,axis=axis,inplace=inplace)\n",
    "        return\n",
    "\n",
    "    def drop_col(self, X=None, colnames=None):\n",
    "        \"\"\"Drops columns\"\"\"\n",
    "        if X is None:\n",
    "            X = self.df\n",
    "            \n",
    "        for column in colnames:\n",
    "            if column in X.columns:\n",
    "                X.drop(columns=column, inplace=True)\n",
    "            else:\n",
    "                print(f\"{column} not in DataFrame.\")     \n",
    "\n",
    "\n",
    "        return\n",
    "\n",
    "    \n",
    "    def add_max_scaled_col(self,colname, group_by = '', prefix='max_scaled_'):\n",
    "        \"\"\"Add max scaled col --> -1 to 1 over a grouping (or not)\"\"\"\n",
    "        new_name = prefix + colname\n",
    "        if len(group_by)==0:\n",
    "            self.df[new_name] = self.df[colname]  / self.df[colname].abs().max()\n",
    "        else:\n",
    "            self.df['max_abs_grouped_' + colname] = self.df.groupby(group_by)[colname].transform(lambda x: x.abs().max())\n",
    "            self.df[new_name] = self.df[colname]/self.df['max_abs_grouped_' + colname] \n",
    "        return\n",
    "\n",
    "    def add_normalized_col(self,colname, prefix='normalized_'):\n",
    "        \"\"\"Add normalized col --> 0 to 1 over a grouping (or not)\"\"\"\n",
    "        new_name = prefix + colname\n",
    "        self.df[new_name] = (self.df[colname] - self.df[colname].min()) / (self.df[colname].max() - self.df[colname].min())\n",
    "        return\n",
    "\n",
    "    def add_standard_col(self,colname, prefix='standard_'):\n",
    "        \"\"\"Add standardized col --> mean 0 standard_dev 1 over a grouping (or not)\"\"\"\n",
    "        new_name = prefix + colname\n",
    "        self.df[new_name] = (self.df[colname] - self.df[colname].mean()) / self.df[colname].std()     \n",
    "        return\n",
    "\n",
    "\n",
    "    def standard_scaling(self, col1, col2):\n",
    "        \"\"\"standardizes two cols \"\"\"\n",
    "        scaler = StandardScaler()\n",
    "        self.df[[col1, col2]] = scaler.fit_transform(self.df[[col1, col2]])\n",
    "\n",
    "    def normalize_scaling(self, col1, col2):\n",
    "        \"\"\"normalizes two cols \"\"\"\n",
    "        scaler = MinMaxScaler()\n",
    "        self.df[[col1, col2]] = scaler.fit_transform(self.df[[col1, col2]])\n",
    "        \n",
    "    def mean(self, colname, group_by='', addCol=False):\n",
    "        \"\"\"Add mean col if addCol=True over a groupby\"\"\"\n",
    "\n",
    "        if group_by:\n",
    "            avg_values = self.df.groupby(group_by)[colname].mean().reset_index()\n",
    "            avg_values.rename(columns={colname: 'mean_' + colname}, inplace=True)\n",
    "            if addCol:\n",
    "                self.df['mean_' + colname + '_group_' + group_by] = self.df.groupby(group_by)[colname].transform('mean')\n",
    "        else:\n",
    "            avg_values = self.df[colname].mean()\n",
    "            if addCol:\n",
    "                self.df['mean_' + colname ] = self.df[colname].transform('mean')\n",
    "\n",
    "            \n",
    "        return avg_values\n",
    "\n",
    "    def max(self, colname, group_by='', addCol=False):\n",
    "        \"\"\"Add max col if addCol=True over a groupby\"\"\"\n",
    "       \n",
    "        if group_by:\n",
    "            max_values = self.df.groupby(group_by)[colname].max().reset_index()\n",
    "            max_values.rename(columns={colname: 'max_' + colname}, inplace=True)\n",
    "            if addCol:\n",
    "                self.df['max_' + colname + '_group_' + group_by] = self.df.groupby(group_by)[colname].transform('max')\n",
    "        else:\n",
    "            if addCol:\n",
    "                self.df['max_' + colname] = self.df[colname].transform('max')\n",
    "            max_values = self.df[colname].max()\n",
    "\n",
    "            return max_values\n",
    "\n",
    "    def min(self, colname, group_by='', addCol=False):\n",
    "        \"\"\"Add min col if addCol=True over a groupby\"\"\"\n",
    "        if addCol:\n",
    "            self.df['min_' + colname + '_group_' + group_by] = self.df.groupby(group_by)[colname].transform('min')\n",
    "\n",
    "        if group_by:\n",
    "            min_values = self.df.groupby(group_by)[colname].min().reset_index()\n",
    "            min_values.rename(columns={colname: 'min_' + colname}, inplace=True)\n",
    "            if addCol:\n",
    "                self.df['min_' + colname + '_group_' + group_by] = self.df.groupby(group_by)[colname].transform('min')\n",
    "\n",
    "        else:\n",
    "            if addCol:\n",
    "                self.df['min_' + colname] = self.df[colname].transform('min')\n",
    "            min_values = self.df[colname].min()\n",
    "\n",
    "        return min_values\n",
    "\n",
    "    def plot_line(self, y='', x=''):\n",
    "        \"\"\" Plots the df with a line. Can specify x and y\"\"\"\n",
    "        plt.figure(figsize=(16,6))\n",
    "        data = self.df\n",
    "        if index:\n",
    "            data = data.set_index(x)    \n",
    "            \n",
    "        if col:\n",
    "            sns.lineplot(data=data[y])        \n",
    "        else:\n",
    "            sns.lineplot(data=data.df)        \n",
    "            \n",
    "        return\n",
    "\n",
    "    def plot_distribution_line(self, col):\n",
    "        \"\"\" Plots column distribution over a line from min to max\"\"\"\n",
    "        plt.figure(figsize=(16,6))\n",
    "        data = self.df[col].sort_values(by=col).reset_index()\n",
    "        sns.lineplot(data=data)\n",
    "        return\n",
    "   \n",
    "   \n",
    "    def distribution(self, cols, num_subplots_perrow=3):\n",
    "        \"\"\" Plots column distribution with histograms. Can specify multiple cols and number of subplots per row\"\"\"\n",
    "        fig, axes = plt.subplots(nrows=int(np.ceil(len(cols)/num_subplots_perrow)),ncols=num_subplots_perrow,figsize=(21,7))\n",
    "        axes = axes.flatten()\n",
    "        for i, column in enumerate(cols):\n",
    "            sns.histplot(self.df[column], kde=True, ax=axes[i])\n",
    "            axes[i].set_title(f'distrib {column}')\n",
    "            axes[i].set_xlabel(column)\n",
    "            axes[i].set_ylabel('Freq')\n",
    "\n",
    "        for j in range(i + 1, len(axes)):\n",
    "            axes[j].axis('off')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    def scatter(self, colx, coly, num_subplots_perrow=1, limit_StdDev=1):\n",
    "        \"\"\"Plots scatter distribution can specify multiple x, one for each subplot, but only one y\n",
    "        can specify limit_percStdDev for restricting values over n standard deviations\"\"\"\n",
    "        if isinstance(colx, str):\n",
    "            colx = [colx]\n",
    "\n",
    "        if num_subplots_perrow>1:\n",
    "            fig, axes = plt.subplots(\n",
    "                nrows=int(np.ceil(len(colx) / num_subplots_perrow)),\n",
    "                ncols=num_subplots_perrow,\n",
    "                figsize=(21, 7)\n",
    "            )\n",
    "    \n",
    "            axes = axes.flatten()\n",
    "        \n",
    "            for i, column in enumerate(colx):\n",
    "                if self.df[column].dtype in ['int64', 'float64'] and self.df[coly].dtype in ['int64', 'float64']:\n",
    "                    sns.scatterplot(data=self.get_percStdDev_data(column, limit_StdDev=limit_StdDev), x=column, y=coly, ax=axes[i], color='b')\n",
    "                    sns.regplot(data=self.get_percStdDev_data(column, limit_StdDev=limit_StdDev), x=column, y=coly, ax=axes[i], scatter=False, color='r', lowess=True)\n",
    "                elif self.df[column].dtype in ['category', 'object'] and self.df[coly].dtype in ['int64', 'float64']:\n",
    "                    sns.boxplot(data=self.df, x=column, y=coly, ax=axes[i], color='b')\n",
    "                else:\n",
    "                    axes[i].text(0.5, 0.5, 'unsupported data', ha='center', va='center', fontsize=12)\n",
    "        \n",
    "                axes[i].set_title(f'Distribuzione di {column}')\n",
    "                axes[i].set_xlabel(column)\n",
    "                axes[i].set_ylabel(coly)\n",
    "        \n",
    "            for j in range(i + 1, len(axes)):\n",
    "                axes[j].axis('off')\n",
    "\n",
    "        else:\n",
    "            for i, column in enumerate(colx):\n",
    "                plt.plot(figsize=(21, 7))\n",
    "                print(f'colx:{column}')\n",
    "                if self.df[column].dtype in ['int64', 'float64'] and self.df[column].dtype in ['int64', 'float64']:\n",
    "                    sns.scatterplot(data=self.df, x=column, y=coly, color='b')\n",
    "                    sns.regplot(data=df, x=column, y=coly,  scatter=False, color='r', lowess=True)\n",
    "                elif self.df[column].dtype in ['category', 'object'] and self.df[coly].dtype in ['int64', 'float64']:\n",
    "                    sns.boxplot(data=self.df, x=column, y=coly,  color='b')\n",
    "    \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def get_percStdDev_data(self, colname, limit_StdDev=1):\n",
    "        \"\"\" Gets data between the number of stdDev specified\"\"\"\n",
    "        mean = self.df[colname].mean()\n",
    "        std_dev = self.df[colname].std()\n",
    "        \n",
    "        inf = mean - (std_dev * limit_StdDev)\n",
    "        sup = mean + (std_dev * limit_StdDev)\n",
    "\n",
    "        df_filtered = self.df[(self.df[colname] >= inf) & (self.df[colname] <= sup)]\n",
    "        return df_filtered\n",
    "\n",
    "\n",
    "    def count_categories(self):\n",
    "        \"\"\" Counts categories cols with their unique values\"\"\"\n",
    "        categories_count = {}\n",
    "        for column in self.df.select_dtypes(include=['object', 'category']).columns:\n",
    "            unique_counts = self.df[column].nunique()\n",
    "            total_counts = len(self.df[column])\n",
    "            categories_count[column] = {'unique': unique_counts}\n",
    "            sorted_results = sorted(categories_count.items(), key=lambda x: x[1]['unique'], reverse=True)\n",
    "    \n",
    "        return sorted_results\n",
    "\n",
    "\n",
    "    def categories_threshold(self, threshold=5, under_threshold=True):\n",
    "        \"\"\" Lists the colnames of categories having unique values under or over a certain threshold\"\"\"\n",
    "        columns_few_unique = []\n",
    "        underOver = 'under'\n",
    "        for column in self.df.select_dtypes(include=['object', 'category']).columns:\n",
    "            unique_count = self.df[column].nunique()  \n",
    "            \n",
    "            if under_threshold:\n",
    "                if unique_count < threshold:  \n",
    "                    columns_few_unique.append(column)\n",
    "            else:\n",
    "                underOver = 'over or equal'\n",
    "                if unique_count >= threshold:  \n",
    "                    columns_few_unique.append(column)\n",
    "                \n",
    "        print(f'Categories with counts {underOver} threshold of {threshold}')\n",
    "        return columns_few_unique\n",
    "\n",
    "\n",
    "    def factorize_categories(self, columns=None):\n",
    "        \"\"\" Factorize the cols (if not specified iterates over all cols) if they do not contain nan\"\"\"\n",
    "        if columns is None:\n",
    "            columns = self.df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        elif isinstance(columns, str):\n",
    "            columns = [columns]\n",
    "        \n",
    "        for column in columns:\n",
    "            if column in self.df.columns:  \n",
    "                if self.df[column].isnull().any():  \n",
    "                    print(f\"Cannot factorize '{column}' containing NaN.\")\n",
    "                else:\n",
    "                    self.df[column] = pd.factorize(self.df[column])[0]  \n",
    "            else:\n",
    "                print(f\"Column '{column}' does not exist in the DataFrame.\")\n",
    "\n",
    "    \n",
    "    def label_encode(self,X=None, columns=None):\n",
    "        \"\"\"Label encode the cols (if not specified iterates over all cols) if they do not contain NaN\"\"\"\n",
    "        if X is None:\n",
    "            X = self.df\n",
    "        \n",
    "        if columns is None:\n",
    "            columns = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        elif isinstance(columns, str):\n",
    "            columns = [columns]\n",
    "        \n",
    "        for column in columns:\n",
    "            if column in X.columns:\n",
    "                if X[column].isnull().any():\n",
    "                    print(f\"Cannot factorize '{column}' containing NaN.\")\n",
    "                else:\n",
    "                    le = LabelEncoder()\n",
    "                    X[column] = le.fit_transform(X[column]) \n",
    "            else:\n",
    "                print(f\"Column '{column}' does not exist in the DataFrame.\")        \n",
    "\n",
    "    \n",
    "    def onehot_encode(self, X=None, columns=None, del_original_col=True):\n",
    "        \"\"\"One-hot encode the specified columns (if not specified, iterates over all categorical columns) if they do not contain NaN.\"\"\"\n",
    "    \n",
    "        if X is None:\n",
    "            X = self.df\n",
    "    \n",
    "        if columns is None:\n",
    "            columns = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        elif isinstance(columns, str):\n",
    "            columns = [columns]\n",
    "        \n",
    "        for column in columns:\n",
    "            if column in X.columns:  \n",
    "                if X[column].isnull().any():  \n",
    "                    print(f\"Cannot factorize '{column}' containing NaN.\")\n",
    "                else:\n",
    "                    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "                    onehot_encoded = onehot_encoder.fit_transform(X[[column]])\n",
    "                    onehot_df = pd.DataFrame(onehot_encoded, columns=onehot_encoder.get_feature_names_out([column]))\n",
    "                    onehot_df.columns = [f\"{column}_{cat}\" for cat in onehot_encoder.categories_[0]]\n",
    "        \n",
    "                    for new_column in onehot_df.columns:\n",
    "                        X[new_column] = onehot_df[new_column]\n",
    "        \n",
    "                    if del_original_col:\n",
    "                        X.drop(column, axis=1, errors='ignore', inplace=True)\n",
    "            else:\n",
    "                print(f\"Column '{column}' does not exist in the DataFrame.\")\n",
    "        \n",
    "        self.df = X  \n",
    "\n",
    "    \n",
    "    \n",
    "    def correlations(self, df=None, annotation=True, limit=0):\n",
    "        \"\"\"\n",
    "        Calculates correlation matrix over numeric cols and shows heatmap. Can tweak with abs limit of correlation\n",
    "        \"\"\"\n",
    "        if df is None:\n",
    "            df = self.df\n",
    "            \n",
    "        numeric_dataframe = df.select_dtypes(include=['number'])\n",
    "        correlation_matrix = numeric_dataframe.corr()\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        filtered_correlation = correlation_matrix[(correlation_matrix.abs() > limit) ]\n",
    "        filtered_correlation = filtered_correlation.where(filtered_correlation.abs() > limit)\n",
    "       \n",
    "        sns.heatmap(filtered_correlation, annot=annotation, fmt=\".2f\", cmap='coolwarm', square=True, cbar=True)\n",
    "        plt.title('Correlation', fontsize=16)\n",
    "        plt.show()        \n",
    "        return\n",
    "\n",
    "\n",
    "    def get_sigma_outliers(self, col):\n",
    "        \"\"\"Returns a series of outliers for the given column. All data out of 3 sigma range\"\"\"\n",
    "        mean = self.df[col].mean()\n",
    "        std_dev = self.df[col].std()\n",
    "        \n",
    "        lower_limit = mean - 3 * std_dev\n",
    "        upper_limit = mean + 3 * std_dev\n",
    "        \n",
    "        outliers = self.df[(self.df[col] < lower_limit) | (self.df[col] > upper_limit)]\n",
    "        return outliers        \n",
    "\n",
    "    def get_iqr_outliers(self, col):\n",
    "        \"\"\"Returns a series of outliers for the given column with inter quartile range\"\"\"\n",
    "        q1 = self.df[col].quantile(0.25)\n",
    "        q3 = self.df[col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        \n",
    "        outlier_indices = self.df[(self.df[col] < lower_bound) | (self.df[col] > upper_bound)]\n",
    "        return outlier_indices\n",
    "\n",
    "    def get_density_outliers(self, selected_columns=None, epsilon=1.5, min_samples=5, metric='euclidean'):\n",
    "        \"\"\"Returns a series of outliers for the given columns with DBSCAN\"\"\"\n",
    "        if selected_columns is None:\n",
    "            selected_columns = self.df.columns\n",
    "\n",
    "        numeric_cols = []\n",
    "        if metric == 'euclidean':\n",
    "            # If metric is euclidean the columns have to be numeric\n",
    "            for col in selected_columns:\n",
    "                if pd.api.types.is_numeric_dtype(self.df[col]):\n",
    "                    numeric_cols.append(col)            \n",
    "            \n",
    "            selected_columns = numeric_cols    \n",
    "        else:\n",
    "            print('metric not recognized by get_density_outliers')\n",
    "            return\n",
    "        \n",
    "        df = self.df[selected_columns]\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        scaled_df = scaler.fit_transform(df)\n",
    "        dbscan = DBSCAN(eps=epsilon, min_samples=min_samples, metric=metric)\n",
    "        labels = dbscan.fit_predict(scaled_df)\n",
    "        outliers = df[labels == -1]\n",
    "        return outliers\n",
    "        \n",
    "    \n",
    "    def remove_outliers(self, col, method='sigma', epsilon=1.5, min_samples=5, metric='euclidean'):\n",
    "        \"\"\"Removes the rows of outliers for the given column\"\"\"\n",
    "        if(method=='sigma'):\n",
    "            outliers = self.get_sigma_outliers(col)\n",
    "        elif(method=='IQR'):\n",
    "            outliers = self.get_iqr_outliers(col)\n",
    "        elif(method=='density'):\n",
    "            outliers = self.get_density_outliers(col)\n",
    "\n",
    "        \n",
    "        self.df = self.df.drop(outliers.index)\n",
    "        self.df.reset_index(drop=True, inplace=True)\n",
    "        return         \n",
    "\n",
    "    def boxplot_outliers(self, col):\n",
    "        \"\"\"Visualize outliers of column with boxplot. Out of wiskers are outliers\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(x=self.df[col])\n",
    "        plt.title(f'Boxplot di {col}')\n",
    "        plt.xlabel(col)\n",
    "        plt.show()    \n",
    "\n",
    "    def pairplot_relations(self, cols=None):\n",
    "        \"\"\"Visualize relations between couples of features\"\"\"\n",
    "        if cols is not None:\n",
    "            sns.pairplot(self.df)\n",
    "            plt.show()\n",
    "        return\n",
    "\n",
    "    def features_importance(self, X=None, y=None, n_features_to_vis=25):\n",
    "        \"\"\"Plots the importance of each feature calculated by RandomForestClassifier\n",
    "        returns importances_df a df of importances\n",
    "        \"\"\"\n",
    "        if X is None:\n",
    "            X = self.df\n",
    "            \n",
    "        if X is not None and y is not None:\n",
    "            model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            model.fit(X, y)\n",
    "            importances = model.feature_importances_\n",
    "\n",
    "            importances_df = pd.DataFrame({\n",
    "                'Feature': X.columns,\n",
    "                'Importance': importances\n",
    "            })\n",
    "\n",
    "            features = np.array(X.columns) \n",
    "            indices = np.argsort(importances)[::-1]\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.title('Feature Importance - Random Forest')\n",
    "            plt.barh(importances_df['Feature'].head(n_features_to_vis), importances[indices][:n_features_to_vis])\n",
    "            plt.xlabel('Importance')\n",
    "            plt.ylabel('Feature')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('feature_importances.png', bbox_inches='tight')\n",
    "            print(importances_df)\n",
    "        else:\n",
    "            print('Specify X and y')\n",
    "        return importances_df\n",
    "        \n",
    "    def mutual_info_scores(self, X=None, y=None):\n",
    "        if X is None:\n",
    "            X = self.get_X()\n",
    "\n",
    "        if y is None:\n",
    "            y = self.get_y()\n",
    "        \n",
    "        for colname in X.select_dtypes([\"object\", \"category\"]):\n",
    "            X[colname], _ = X[colname].factorize()\n",
    "\n",
    "        # All discrete features should now have integer dtypes\n",
    "        discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n",
    "        mi_scores = mutual_info_regression(X, y)\n",
    "        mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "        mi_scores = mi_scores.sort_values(ascending=False)\n",
    "        return mi_scores\n",
    "\n",
    "    \n",
    "    def plot_mutual_info_scores(self,scores):\n",
    "        scores = scores.sort_values(ascending=True)\n",
    "        width = np.arange(len(scores))\n",
    "        ticks = list(scores.index)\n",
    "        plt.barh(width, scores)\n",
    "        plt.yticks(width, ticks)\n",
    "        plt.yticks(rotation=45)  \n",
    "        plt.title(\"Mutual Information Scores\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_csv(file_path, parse_dates=True):\n",
    "    df = pd.read_csv(file_path,  parse_dates=parse_dates)\n",
    "    handler = PandasHandler(df)\n",
    "    handler.set_features(df.columns.tolist())\n",
    "    return handler\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.581061,
   "end_time": "2025-02-07T15:08:23.755942",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-07T15:08:17.174881",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
