{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3068743c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-03T14:37:05.289810Z",
     "iopub.status.busy": "2025-02-03T14:37:05.289451Z",
     "iopub.status.idle": "2025-02-03T14:37:06.384855Z",
     "shell.execute_reply": "2025-02-03T14:37:06.383582Z"
    },
    "papermill": {
     "duration": 1.101601,
     "end_time": "2025-02-03T14:37:06.387315",
     "exception": false,
     "start_time": "2025-02-03T14:37:05.285714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e270647f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T14:37:06.396728Z",
     "iopub.status.busy": "2025-02-03T14:37:06.396079Z",
     "iopub.status.idle": "2025-02-03T14:37:08.563601Z",
     "shell.execute_reply": "2025-02-03T14:37:08.562399Z"
    },
    "papermill": {
     "duration": 2.17541,
     "end_time": "2025-02-03T14:37:08.566405",
     "exception": false,
     "start_time": "2025-02-03T14:37:06.390995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class PandasHandler:\n",
    "    \"\"\"\n",
    "    Handler for basic operations on dataframe pandas\n",
    "    visualization, nan inspection and repair\n",
    "    LOADING OF Dataframe\n",
    "    load_csv(file_path, parse_dates=True):\n",
    "    def get_df(self, columns=''):\n",
    "    \n",
    "    # Investigation\n",
    "    def get_col_with_nan(self):\n",
    "    def get_numeric_cols(self):\n",
    "    def get_text_cols(self):    \n",
    "    def get_categoric_cols(self):\n",
    "    def display_col_with_nan(self):\n",
    "    def get_percStdDev_data(self, colname, limit_percStdDev=1):\n",
    "    def count_categories(self):\n",
    "\n",
    "    # Repair\n",
    "    def repair_nan(self, colname=None, mode='value', value=0):\n",
    "    def drop_row_with_nan_val(self, threshold=1,axis=0,inplace=True):\n",
    "    def drop_col(self,colnames):\n",
    "    def categories_threshold(self, threshold=5, under_threshold=True):\n",
    "    def factorize_categories(self, columns=None):\n",
    "\n",
    "    # Add utility cols\n",
    "    def add_max_scaled_col(self,colname, group_by = '', prefix='max_scaled_'):\n",
    "    def add_normalized_col(self,colname, prefix='normalized_'):\n",
    "    def add_standard_col(self,colname, prefix='standard_'):\n",
    "    def mean(self, colname, group_by='', addCol=False):\n",
    "    def max(self, colname, group_by='', addCol=False):\n",
    "    def min(self, colname, group_by='', addCol=False):\n",
    "\n",
    "    # Visualization\n",
    "    def plot_line(self, y='', x=''):\n",
    "    def plot_distribution_line(self, col):\n",
    "    def distribution(self, cols, num_subplots_perrow=3):\n",
    "    def scatter(self, colx, coly, num_subplots_perrow=3, limit_percStdDev=1):\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.df = pd.DataFrame(data)\n",
    "        self.cols_containing_NaN = self._get_columns_with_nan()\n",
    "    \n",
    "    def _get_columns_with_nan(self):\n",
    "        return self.df.columns[self.df.isna().any()].tolist()\n",
    "\n",
    "    def get_df(self, columns=''):\n",
    "        \"\"\"Gets the dataframe\"\"\"\n",
    "        if not columns:\n",
    "            return self.df\n",
    "        else:\n",
    "            return self.df[columns]\n",
    "\n",
    "    def get_col_with_nan(self):\n",
    "        \"\"\"Gets all column names with nan and lists them with counting\"\"\"\n",
    "        print(self._get_columns_with_nan())\n",
    "        nan_columns = {}\n",
    "        total = self.df.shape[0]\n",
    "        \n",
    "        for column in self.df.columns:\n",
    "            nan_count = self.df[column].isnull().sum()\n",
    "            if nan_count > 0:\n",
    "                percentage = '{:.2f}'.format(nan_count*100 / total)\n",
    "                nan_columns[column] = f'{nan_count} count, {percentage}% of total'\n",
    "        \n",
    "        return nan_columns\n",
    "        \n",
    "    def repair_nan(self, colname=None, mode='value', value=0):\n",
    "        \"\"\"\n",
    "        Repair df where nan is found in columns \n",
    "        MODES: value, delrow, mostcommon, mean, min, max, random_number\n",
    "        \"\"\"\n",
    "        if colname is None:\n",
    "            colname = self.df.columns.tolist()\n",
    "\n",
    "        if isinstance(colname, str):\n",
    "            colname = [colname]\n",
    "\n",
    "        for col in colname:\n",
    "            if col not in self.df.columns:\n",
    "                print(f\"{col} not found DataFrame.\")\n",
    "                continue\n",
    "\n",
    "            if mode == 'value':\n",
    "                self.df[col].fillna(value, inplace=True)\n",
    "            elif mode == 'delrow':\n",
    "                self.df.dropna(subset=[col], inplace=True)\n",
    "            elif mode == 'mostcommon':\n",
    "                most_common = self.df[col].mode()[0] \n",
    "                print(f'mostcommon {most_common}')\n",
    "                self.df[col].fillna(most_common, inplace=True)\n",
    "            elif mode == 'mean':\n",
    "                mean_value = self.df[col].mean()\n",
    "                self.df[col].fillna(mean_value, inplace=True)\n",
    "            elif mode == 'min':\n",
    "                min_value = self.df[col].min()\n",
    "                self.df[col].fillna(min_value, inplace=True)\n",
    "            elif mode == 'max':\n",
    "                max_value = self.df[col].max()\n",
    "                self.df[col].fillna(max_value, inplace=True)\n",
    "            elif mode == 'random_number':\n",
    "                mean_value = self.df[col].mean()\n",
    "                std_dev = self.df[col].std()  \n",
    "                def is_nan(x):\n",
    "                    return pd.isna(x)\n",
    "                \n",
    "                def random_error(x):\n",
    "                    if is_nan(x):\n",
    "                        return random.gauss(mean_value, 2 * std_dev) \n",
    "                    return x\n",
    "            \n",
    "                self.df[col] = self.df[col].apply(random_error)            \n",
    "            else:\n",
    "                print(f'{mode} not found')\n",
    "\n",
    "        return\n",
    "\n",
    "    def get_numeric_cols(self):\n",
    "        \"\"\"Gets all numeric column names in a list\"\"\"\n",
    "        return self.df.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "    def get_text_cols(self):\n",
    "        \"\"\"Gets all text column names in a list\"\"\"\n",
    "        return self.df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    def get_categoric_cols(self):\n",
    "        \"\"\"Gets all categoric column names in a list\"\"\"\n",
    "        return self.df.select_dtypes(include=['category']).columns.tolist()\n",
    "    \n",
    "    def display_col_with_nan(self):\n",
    "        \"\"\"Displays total of nan counts, the columns containing nan\"\"\"\n",
    "        nan_count = self.df.isnull().sum()\n",
    "        print('total NaN:' + str(nan_count.sum()))\n",
    "        print(f'Fields containing NaN:    {nan_count[nan_count>0].sort_values(ascending=False)}')\n",
    "        return self.cols_containing_NaN\n",
    "    \n",
    "    def drop_row_with_nan_val(self, threshold=1,axis=0,inplace=True):\n",
    "        \"\"\"Drops rows having a nan count over the cols over a certain threshold. Ex:threshold=1\n",
    "        mantains only rows containing 1 nan\"\"\"\n",
    "        self.df.dropna(threshold=threshold,axis=axis,inplace=inplace)\n",
    "        return\n",
    "\n",
    "    def drop_col(self,colnames):\n",
    "        \"\"\"Drops columns\"\"\"\n",
    "        self.df = self.df.drop(columns=colnames)\n",
    "        return\n",
    "\n",
    "\n",
    "    \n",
    "    def add_max_scaled_col(self,colname, group_by = '', prefix='max_scaled_'):\n",
    "        \"\"\"Add max scaled col --> -1 to 1 over a grouping (or not)\"\"\"\n",
    "        new_name = prefix + colname\n",
    "        if len(group_by)==0:\n",
    "            self.df[new_name] = self.df[colname]  / self.df[colname].abs().max()\n",
    "        else:\n",
    "            self.df['max_abs_grouped_' + colname] = self.df.groupby(group_by)[colname].transform(lambda x: x.abs().max())\n",
    "            self.df[new_name] = self.df[colname]/self.df['max_abs_grouped_' + colname] \n",
    "        return\n",
    "\n",
    "    def add_normalized_col(self,colname, prefix='normalized_'):\n",
    "        \"\"\"Add normalized col --> 0 to 1 over a grouping (or not)\"\"\"\n",
    "        new_name = prefix + colname\n",
    "        self.df[new_name] = (self.df[colname] - self.df[colname].min()) / (self.df[colname].max() - self.df[colname].min())\n",
    "        return\n",
    "\n",
    "    def add_standard_col(self,colname, prefix='standard_'):\n",
    "        \"\"\"Add standardized col --> mean 0 standard_dev 1 over a grouping (or not)\"\"\"\n",
    "        new_name = prefix + colname\n",
    "        self.df[new_name] = (self.df[colname] - self.df[colname].mean()) / self.df[colname].std()     \n",
    "        return\n",
    "\n",
    "    def mean(self, colname, group_by='', addCol=False):\n",
    "        \"\"\"Add mean col if addCol=True over a groupby\"\"\"\n",
    "\n",
    "        if group_by:\n",
    "            avg_values = self.df.groupby(group_by)[colname].mean().reset_index()\n",
    "            avg_values.rename(columns={colname: 'mean_' + colname}, inplace=True)\n",
    "            if addCol:\n",
    "                self.df['mean_' + colname + '_group_' + group_by] = self.df.groupby(group_by)[colname].transform('mean')\n",
    "        else:\n",
    "            avg_values = self.df[colname].mean()\n",
    "            if addCol:\n",
    "                self.df['mean_' + colname ] = self.df[colname].transform('mean')\n",
    "\n",
    "            \n",
    "        return avg_values\n",
    "\n",
    "    def max(self, colname, group_by='', addCol=False):\n",
    "        \"\"\"Add max col if addCol=True over a groupby\"\"\"\n",
    "       \n",
    "        if group_by:\n",
    "            max_values = self.df.groupby(group_by)[colname].max().reset_index()\n",
    "            max_values.rename(columns={colname: 'max_' + colname}, inplace=True)\n",
    "            if addCol:\n",
    "                self.df['max_' + colname + '_group_' + group_by] = self.df.groupby(group_by)[colname].transform('max')\n",
    "        else:\n",
    "            if addCol:\n",
    "                self.df['max_' + colname] = self.df[colname].transform('max')\n",
    "            max_values = self.df[colname].max()\n",
    "\n",
    "            return max_values\n",
    "\n",
    "    def min(self, colname, group_by='', addCol=False):\n",
    "        \"\"\"Add min col if addCol=True over a groupby\"\"\"\n",
    "        if addCol:\n",
    "            self.df['min_' + colname + '_group_' + group_by] = self.df.groupby(group_by)[colname].transform('min')\n",
    "\n",
    "        if group_by:\n",
    "            min_values = self.df.groupby(group_by)[colname].min().reset_index()\n",
    "            min_values.rename(columns={colname: 'min_' + colname}, inplace=True)\n",
    "            if addCol:\n",
    "                self.df['min_' + colname + '_group_' + group_by] = self.df.groupby(group_by)[colname].transform('min')\n",
    "\n",
    "        else:\n",
    "            if addCol:\n",
    "                self.df['min_' + colname] = self.df[colname].transform('min')\n",
    "            min_values = self.df[colname].min()\n",
    "\n",
    "        return min_values\n",
    "\n",
    "    def plot_line(self, y='', x=''):\n",
    "        \"\"\" Plots the df with a line. Can specify x and y\"\"\"\n",
    "        plt.figure(figsize=(16,6))\n",
    "        data = self.df\n",
    "        if index:\n",
    "            data = data.set_index(x)    \n",
    "            \n",
    "        if col:\n",
    "            sns.lineplot(data=data[y])        \n",
    "        else:\n",
    "            sns.lineplot(data=data.df)        \n",
    "            \n",
    "        return\n",
    "\n",
    "    def plot_distribution_line(self, col):\n",
    "        \"\"\" Plots column distribution over a line from min to max\"\"\"\n",
    "        plt.figure(figsize=(16,6))\n",
    "        data = self.df[col].sort_values(by=col).reset_index()\n",
    "        sns.lineplot(data=data)\n",
    "        return\n",
    "   \n",
    "   \n",
    "    def distribution(self, cols, num_subplots_perrow=3):\n",
    "        \"\"\" Plots column distribution with histograms. Can specify multiple cols and number of subplots per row\"\"\"\n",
    "        fig, axes = plt.subplots(nrows=int(np.ceil(len(cols)/num_subplots_perrow)),ncols=num_subplots_perrow,figsize=(21,7))\n",
    "        axes = axes.flatten()\n",
    "        for i, column in enumerate(cols):\n",
    "            sns.histplot(self.df[column], kde=True, ax=axes[i])\n",
    "            axes[i].set_title(f'distrib {column}')\n",
    "            axes[i].set_xlabel(column)\n",
    "            axes[i].set_ylabel('Freq')\n",
    "\n",
    "        for j in range(i + 1, len(axes)):\n",
    "            axes[j].axis('off')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    def scatter(self, colx, coly, num_subplots_perrow=3, limit_StdDev=1):\n",
    "        \"\"\"Plots scatter distribution can specify multiple x, one for each subplot, but only one y\n",
    "        can specify limit_percStdDev for restricting values over n standard deviations\"\"\"\n",
    "        if isinstance(colx, str):\n",
    "            colx = [colx]\n",
    "    \n",
    "        fig, axes = plt.subplots(\n",
    "            nrows=int(np.ceil(len(colx) / num_subplots_perrow)),\n",
    "            ncols=num_subplots_perrow,\n",
    "            figsize=(21, 7)\n",
    "        )\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "        for i, column in enumerate(colx):\n",
    "            if self.df[column].dtype in ['int64', 'float64'] and self.df[coly].dtype in ['int64', 'float64']:\n",
    "                sns.scatterplot(data=self.get_percStdDev_data(column, limit_StdDev=limit_StdDev), x=column, y=coly, ax=axes[i], color='b')\n",
    "                sns.regplot(data=self.get_percStdDev_data(column, limit_StdDev=limit_StdDev), x=column, y=coly, ax=axes[i], scatter=False, color='r', lowess=True)\n",
    "            elif self.df[column].dtype in ['category', 'object'] and self.df[coly].dtype in ['int64', 'float64']:\n",
    "                sns.boxplot(data=self.df, x=column, y=coly, ax=axes[i], color='b')\n",
    "            else:\n",
    "                axes[i].text(0.5, 0.5, 'unsupported data', ha='center', va='center', fontsize=12)\n",
    "    \n",
    "            axes[i].set_title(f'Distribuzione di {column}')\n",
    "            axes[i].set_xlabel(column)\n",
    "            axes[i].set_ylabel(coly)\n",
    "    \n",
    "        for j in range(i + 1, len(axes)):\n",
    "            axes[j].axis('off')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def get_percStdDev_data(self, colname, limit_StdDev=1):\n",
    "        \"\"\" Gets data between the number of stdDev specified\"\"\"\n",
    "        mean = self.df[colname].mean()\n",
    "        std_dev = self.df[colname].std()\n",
    "        \n",
    "        inf = mean - (std_dev * limit_StdDev)\n",
    "        sup = mean + (std_dev * limit_StdDev)\n",
    "\n",
    "        df_filtered = self.df[(self.df[colname] >= inf) & (self.df[colname] <= sup)]\n",
    "        return df_filtered\n",
    "\n",
    "\n",
    "    def count_categories(self):\n",
    "        \"\"\" Counts categories cols with their unique values\"\"\"\n",
    "        categories_count = {}\n",
    "        for column in self.df.select_dtypes(include=['object', 'category']).columns:\n",
    "            unique_counts = self.df[column].nunique()\n",
    "            total_counts = len(self.df[column])\n",
    "            categories_count[column] = {'unique': unique_counts}\n",
    "            sorted_results = sorted(categories_count.items(), key=lambda x: x[1]['unique'], reverse=True)\n",
    "    \n",
    "        return sorted_results\n",
    "\n",
    "\n",
    "    def categories_threshold(self, threshold=5, under_threshold=True):\n",
    "        \"\"\" Lists the colnames of categories having unique values under or over a certain threshold\"\"\"\n",
    "        columns_few_unique = []\n",
    "        underOver = 'under'\n",
    "        for column in self.df.select_dtypes(include=['object', 'category']).columns:\n",
    "            unique_count = self.df[column].nunique()  \n",
    "            \n",
    "            if under_threshold:\n",
    "                if unique_count < threshold:  \n",
    "                    columns_few_unique.append(column)\n",
    "            else:\n",
    "                underOver = 'over or equal'\n",
    "                if unique_count >= threshold:  \n",
    "                    columns_few_unique.append(column)\n",
    "                \n",
    "        print(f'Categories with counts {underOver} threshold of {threshold}')\n",
    "        return columns_few_unique\n",
    "\n",
    "\n",
    "    def factorize_categories(self, columns=None):\n",
    "        \"\"\" Factorize the cols (if not specified iterates over all cols) if they do not contain nan\"\"\"\n",
    "        if columns is None:\n",
    "            columns = self.df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        elif isinstance(columns, str):\n",
    "            columns = [columns]\n",
    "        \n",
    "        for column in columns:\n",
    "            if column in self.df.columns:  \n",
    "                if self.df[column].isnull().any():  \n",
    "                    print(f\"Cannot factorize '{column}' containing NaN.\")\n",
    "                else:\n",
    "                    self.df[column] = pd.factorize(self.df[column])[0]  \n",
    "            else:\n",
    "                print(f\"Column '{column}' does not exist in the DataFrame.\")\n",
    "                \n",
    "\n",
    "\n",
    "def load_csv(file_path, parse_dates=True):\n",
    "    df = pd.read_csv(file_path,  parse_dates=parse_dates)\n",
    "    handler = PandasHandler(df)\n",
    "    return handler\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.730272,
   "end_time": "2025-02-03T14:37:09.291094",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-03T14:37:02.560822",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
